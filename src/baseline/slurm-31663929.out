/nas/longleaf/home/dieckhau/miniconda3/envs/sandbox/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.
  warnings.warn(
/nas/longleaf/home/dieckhau/miniconda3/envs/sandbox/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  warnings.warn(
Curated dataset sizes:
Selection:	 90190 
Antiselection:	 94613
(184803, 7) 
 Index(['ORF', 'round', 'seq', 'count', 'pct', 'var_seq', 'active'], dtype='object')
Running model logreg
Accuracy:	0.72
F1_score:	0.71
Precision:	0.72
Recall:	0.7
TP: 6368 
FP: 2501 
TN: 6946 
FN: 2666
{'Accuracy': 0.7204155619284671, 'F1_score': 0.7113891526559795, 'Precision': 0.7180065396324276, 'Recall': 0.7048926278503431, 'TN': 6946, 'FP': 2501, 'FN': 2666, 'TP': 6368}
Completed model logreg
==================================================
Running model ridge
Accuracy:	0.72
F1_score:	0.71
Precision:	0.72
Recall:	0.7
TP: 6365 
FP: 2523 
TN: 6924 
FN: 2669
{'Accuracy': 0.7190628212759049, 'F1_score': 0.7103001897109698, 'Precision': 0.7161341134113411, 'Recall': 0.7045605490369714, 'TN': 6924, 'FP': 2523, 'FN': 2669, 'TP': 6365}
Completed model ridge
==================================================
Running model svc
Accuracy:	0.72
F1_score:	0.71
Precision:	0.72
Recall:	0.71
TP: 6371 
FP: 2508 
TN: 6939 
FN: 2663
{'Accuracy': 0.7201991234240571, 'F1_score': 0.7113269692402168, 'Precision': 0.7175357585313662, 'Recall': 0.7052247066637148, 'TN': 6939, 'FP': 2508, 'FN': 2663, 'TP': 6371}
Completed model svc
==================================================
Running model rf
Accuracy:	0.72
F1_score:	0.71
Precision:	0.72
Recall:	0.7
TP: 6346 
FP: 2497 
TN: 6950 
FN: 2688
{'Accuracy': 0.7194415886586224, 'F1_score': 0.7099625216758965, 'Precision': 0.7176297636548683, 'Recall': 0.7024573832189507, 'TN': 6950, 'FP': 2497, 'FN': 2688, 'TP': 6346}
Completed model rf
==================================================
Running model gb
Accuracy:	0.71
F1_score:	0.7
Precision:	0.71
Recall:	0.7
TP: 6311 
FP: 2600 
TN: 6847 
FN: 2723
{'Accuracy': 0.7119744602564796, 'F1_score': 0.7033714126497631, 'Precision': 0.7082257883514756, 'Recall': 0.6985831303962807, 'TN': 6847, 'FP': 2600, 'FN': 2723, 'TP': 6311}
Completed model gb
==================================================
Running model ada
Accuracy:	0.71
F1_score:	0.7
Precision:	0.71
Recall:	0.69
TP: 6236 
FP: 2574 
TN: 6873 
FN: 2798
{'Accuracy': 0.709323088577458, 'F1_score': 0.6989464245684824, 'Precision': 0.7078320090805902, 'Recall': 0.690281160061988, 'TN': 6873, 'FP': 2574, 'FN': 2798, 'TP': 6236}
Completed model ada
==================================================
Running model mlp
Iteration 1, loss = 0.58441872
Iteration 2, loss = 0.56714466
Iteration 3, loss = 0.55842967
Iteration 4, loss = 0.55186824
Iteration 5, loss = 0.54548655
Iteration 6, loss = 0.53932856
Iteration 7, loss = 0.53303575
Iteration 8, loss = 0.52621758
Iteration 9, loss = 0.51907268
Iteration 10, loss = 0.51184601
Iteration 11, loss = 0.50420287
Iteration 12, loss = 0.49672165
Iteration 13, loss = 0.48927257
Iteration 14, loss = 0.48238694
Iteration 15, loss = 0.47517424
Iteration 16, loss = 0.46805955
Iteration 17, loss = 0.46137671
Iteration 18, loss = 0.45545763
Iteration 19, loss = 0.44916544
Iteration 20, loss = 0.44294521
Iteration 21, loss = 0.43664925
Iteration 22, loss = 0.43127517
Iteration 23, loss = 0.42647302
Iteration 24, loss = 0.42117367
Iteration 25, loss = 0.41593064
Iteration 26, loss = 0.41025638
Iteration 27, loss = 0.40554903
Iteration 28, loss = 0.40149452
Iteration 29, loss = 0.39605684
Iteration 30, loss = 0.39246199
Iteration 31, loss = 0.38800175
Iteration 32, loss = 0.38333142
Iteration 33, loss = 0.37970744
Iteration 34, loss = 0.37585396
Iteration 35, loss = 0.37253455
Iteration 36, loss = 0.36778977
Iteration 37, loss = 0.36407805
Iteration 38, loss = 0.36030865
Iteration 39, loss = 0.35729700
Iteration 40, loss = 0.35464721
Iteration 41, loss = 0.35117959
Iteration 42, loss = 0.34720618
Iteration 43, loss = 0.34380243
Iteration 44, loss = 0.34160923
Iteration 45, loss = 0.33798147
Iteration 46, loss = 0.33501379
Iteration 47, loss = 0.33201290
Iteration 48, loss = 0.32903484
Iteration 49, loss = 0.32630326
Iteration 50, loss = 0.32444111
Iteration 51, loss = 0.32084552
Iteration 52, loss = 0.31927868
Iteration 53, loss = 0.31593072
Iteration 54, loss = 0.31290204
Iteration 55, loss = 0.31112887
Iteration 56, loss = 0.30994716
Iteration 57, loss = 0.30686868
Iteration 58, loss = 0.30380877
Iteration 59, loss = 0.30247347
Iteration 60, loss = 0.30039871
Iteration 61, loss = 0.29887782
Iteration 62, loss = 0.29534873
Iteration 63, loss = 0.29400781
Iteration 64, loss = 0.29177979
Iteration 65, loss = 0.29033049
Iteration 66, loss = 0.28868243
Iteration 67, loss = 0.28670205
Iteration 68, loss = 0.28461461
Iteration 69, loss = 0.28193865
Iteration 70, loss = 0.28154770
Iteration 71, loss = 0.27945211
Iteration 72, loss = 0.27744055
Iteration 73, loss = 0.27534610
Iteration 74, loss = 0.27524160
Iteration 75, loss = 0.27187665
Iteration 76, loss = 0.27143975
Iteration 77, loss = 0.26978401
Iteration 78, loss = 0.26843581
Iteration 79, loss = 0.26685781
Iteration 80, loss = 0.26530388
Iteration 81, loss = 0.26437098
Iteration 82, loss = 0.26190157
Iteration 83, loss = 0.26102005
Iteration 84, loss = 0.25925945
Iteration 85, loss = 0.25778622
Iteration 86, loss = 0.25746032
Iteration 87, loss = 0.25538372
Iteration 88, loss = 0.25338098
Iteration 89, loss = 0.25172984
Iteration 90, loss = 0.25075280
Iteration 91, loss = 0.24864138
Iteration 92, loss = 0.24848202
Iteration 93, loss = 0.24664256
Iteration 94, loss = 0.24580406
Iteration 95, loss = 0.24352204
Iteration 96, loss = 0.24306582
Iteration 97, loss = 0.24155052
Iteration 98, loss = 0.23938026
Iteration 99, loss = 0.24048845
Iteration 100, loss = 0.23755919
Iteration 101, loss = 0.23721957
Iteration 102, loss = 0.23667034
Iteration 103, loss = 0.23480234
Iteration 104, loss = 0.23384497
Iteration 105, loss = 0.23311711
Iteration 106, loss = 0.23191344
Iteration 107, loss = 0.23095468
Iteration 108, loss = 0.22961231
Iteration 109, loss = 0.22882100
Iteration 110, loss = 0.22726994
Iteration 111, loss = 0.22801106
Iteration 112, loss = 0.22569918
Iteration 113, loss = 0.22420012
Iteration 114, loss = 0.22415122
Iteration 115, loss = 0.22294765
Iteration 116, loss = 0.22198563
Iteration 117, loss = 0.22211006
Iteration 118, loss = 0.21975948
Iteration 119, loss = 0.21832484
Iteration 120, loss = 0.21860580
Iteration 121, loss = 0.21848698
Iteration 122, loss = 0.21593660
Iteration 123, loss = 0.21713492
Iteration 124, loss = 0.21607917
Iteration 125, loss = 0.21376683
Iteration 126, loss = 0.21413331
Iteration 127, loss = 0.21253092
Iteration 128, loss = 0.21128940
Iteration 129, loss = 0.21045180
Iteration 130, loss = 0.21050865
Iteration 131, loss = 0.20889330
Iteration 132, loss = 0.20843021
Iteration 133, loss = 0.20727432
Iteration 134, loss = 0.20718965
Iteration 135, loss = 0.20578496
Iteration 136, loss = 0.20555758
Iteration 137, loss = 0.20477124
Iteration 138, loss = 0.20281787
Iteration 139, loss = 0.20443577
Iteration 140, loss = 0.20359298
Iteration 141, loss = 0.20129887
Iteration 142, loss = 0.20160422
Iteration 143, loss = 0.20098920
Iteration 144, loss = 0.19867820
Iteration 145, loss = 0.19925948
Iteration 146, loss = 0.19774112
Iteration 147, loss = 0.19737554
Iteration 148, loss = 0.19658301
Iteration 149, loss = 0.19711144
Iteration 150, loss = 0.19456521
Iteration 151, loss = 0.19455490
Iteration 152, loss = 0.19455995
Iteration 153, loss = 0.19374865
Iteration 154, loss = 0.19186899
Iteration 155, loss = 0.19308958
Iteration 156, loss = 0.19112655
Iteration 157, loss = 0.19042798
Iteration 158, loss = 0.18994490
Iteration 159, loss = 0.18943650
Iteration 160, loss = 0.18903899
Iteration 161, loss = 0.18829925
Iteration 162, loss = 0.18872506
Iteration 163, loss = 0.18661888
Iteration 164, loss = 0.18582326
Iteration 165, loss = 0.18675523
Iteration 166, loss = 0.18598552
Iteration 167, loss = 0.18544515
Iteration 168, loss = 0.18340479
Iteration 169, loss = 0.18273374
Iteration 170, loss = 0.18235910
Iteration 171, loss = 0.18250170
Iteration 172, loss = 0.18307343
Iteration 173, loss = 0.18180796
Iteration 174, loss = 0.18146301
Iteration 175, loss = 0.18029164
Iteration 176, loss = 0.17916831
Iteration 177, loss = 0.17953005
Iteration 178, loss = 0.17853762
Iteration 179, loss = 0.17813433
Iteration 180, loss = 0.17854297
Iteration 181, loss = 0.17673060
Iteration 182, loss = 0.17679685
Iteration 183, loss = 0.17532489
Iteration 184, loss = 0.17719367
Iteration 185, loss = 0.17401302
Iteration 186, loss = 0.17313637
Iteration 187, loss = 0.17421550
Iteration 188, loss = 0.17375862
Iteration 189, loss = 0.17392291
Iteration 190, loss = 0.17289312
Iteration 191, loss = 0.17133496
Iteration 192, loss = 0.17240209
Iteration 193, loss = 0.17060800
Iteration 194, loss = 0.17098620
Iteration 195, loss = 0.16830252
Iteration 196, loss = 0.16932946
Iteration 197, loss = 0.16843646
Iteration 198, loss = 0.16951469
Iteration 199, loss = 0.16764818
Iteration 200, loss = 0.16900739
Accuracy:	0.63
F1_score:	0.62
Precision:	0.63
Recall:	0.6
TP: 5453 
FP: 3207 
TN: 6240 
FN: 3581
{'Accuracy': 0.6327038580163411, 'F1_score': 0.6163671301005991, 'Precision': 0.629676674364896, 'Recall': 0.6036085897719725, 'TN': 6240, 'FP': 3207, 'FN': 3581, 'TP': 5453}
Completed model mlp
==================================================
