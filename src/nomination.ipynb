{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dabc305d",
   "metadata": {},
   "source": [
    "# Code for nomination peptide generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455e2a6b-c329-474f-83a6-e448183237ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from analysis import screen_routine\n",
    "from dataset import SequenceDataset\n",
    "from utils import hamming_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfad389",
   "metadata": {},
   "source": [
    "## Shared functions used by multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3872850a-fa3e-4ee2-a1c2-5e81f4e37f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming(args, library, pred_list):\n",
    "    \"\"\"Filter library based on hamming cutoff\"\"\"\n",
    "    if args.hamming > 0:\n",
    "        ref = SequenceDataset(sele=args.sele, anti=args.anti, features=args.features, variable_region=args.variable_region)\n",
    "        tr_peps = ref.df.var_seq.values.astype('str')\n",
    "        tr_peps = np.array([list(word) for word in tr_peps])\n",
    "        flag, min_H, closest = [], [], []\n",
    "        library = np.array([list(word) for word in library])\n",
    "        for pep in tqdm(library):\n",
    "            distances = hamming_distance(P=tr_peps, pep=pep, return_distance=True)\n",
    "            min_dist = np.min(distances)\n",
    "            flag.append(min_dist > args.hamming)\n",
    "            min_H.append(min_dist)\n",
    "            sim_seq = tr_peps[np.argmin(distances), :]\n",
    "            closest.append(sim_seq)\n",
    "        \n",
    "        # library = np.array([list(word) for word in library])\n",
    "        library = library[flag, :]\n",
    "        pred_list = torch.tensor(pred_list).detach().numpy()[flag]\n",
    "        min_H = np.array(min_H)[flag]\n",
    "        closest = np.array(closest)[flag]\n",
    "    \n",
    "        library = np.array([''.join(l) for l in library])\n",
    "        closest = np.array([''.join(l) for l in closest])\n",
    "        df = pd.DataFrame({\n",
    "            'SEQ': library, \n",
    "            'MODEL_SCORE': pred_list, \n",
    "            'MIN_HAMMING_DIST': min_H, \n",
    "            'MOST_SIMILAR_TRAINING_SEQ': closest\n",
    "        })\n",
    "    \n",
    "        print('%s peptides passed Hamming cutoff' % str(df.shape[0]))\n",
    "        return df, ref\n",
    "    else:\n",
    "        ref = SequenceDataset(sele=args.sele, anti=args.anti, features=args.features, variable_region=args.variable_region)\n",
    "        return ref\n",
    "\n",
    "def y_star_score(sel_df, anti_df):\n",
    "    \"\"\"Calculate Y-star scores for a sele/anti dataset\"\"\"\n",
    "    \n",
    "    def y_star_single(sel_df, anti_df, p, aa):\n",
    "        alphabet = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "        f_sel = sel_df.loc[sel_df['var_seq'].str[p] == alphabet[aa]]['count'].sum() / sel_df['count'].sum()\n",
    "        f_anti = anti_df.loc[anti_df['var_seq'].str[p] == alphabet[aa]]['count'].sum() / anti_df['count'].sum()\n",
    "        return float(f_sel) / float(f_anti)\n",
    "\n",
    "    seqs = sel_df['var_seq'].values\n",
    "    pos_idx = len(seqs[0])\n",
    "    aa_idx = 20\n",
    "    y_star = np.zeros((pos_idx, aa_idx), dtype=np.float32)\n",
    "    for p in tqdm(range(pos_idx)):\n",
    "        for aa in range(aa_idx):\n",
    "            y_star[p, aa] = y_star_single(sel_df, anti_df, p, aa)\n",
    "    return y_star\n",
    "\n",
    "def s_score(df, y_star):\n",
    "    \"\"\"Calculate S Score for a df of seqs\"\"\"\n",
    "    alphabet = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "    y_star = np.log2(y_star)\n",
    "    seqs = df['var_seq']\n",
    "    s_scores = []\n",
    "    for s in tqdm(seqs):\n",
    "        y_list = []\n",
    "        flag = True\n",
    "        for idx, aa in enumerate(s):\n",
    "            try:\n",
    "                y = y_star[idx, alphabet.index(aa)]\n",
    "                y_list.append(y)\n",
    "            except ValueError:\n",
    "                flag = False\n",
    "                break\n",
    "        if flag:\n",
    "            s_scores.append(np.sum(y_list))\n",
    "    return s_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd7cdf",
   "metadata": {},
   "source": [
    "# Peptide set filtering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50dd9eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_2Cys(library, pred_list):\n",
    "    \"\"\"Filter library to only include those with 2 Cys (also remove problematic residues, KR\"\"\"\n",
    "    seqs = [''.join(l) for l in library]\n",
    "    # seq_filter = [(s[4:].count('C') == 2 and s.count('K') < 1 and s.count('R') < 1) for s in seqs]\n",
    "    seq_filter = [(s[4:].count('C') == 2) for s in seqs]\n",
    "\n",
    "    plist, slist = [], []\n",
    "    for s, f, p in zip(seqs, seq_filter, pred_list):\n",
    "        if f:\n",
    "            slist.append(s)\n",
    "            plist.append(p)\n",
    "    \n",
    "    # filter out any with K or R included\n",
    "    print('%s seqs passed flagged residue filters' % str(len(slist)))\n",
    "    library = slist\n",
    "    pred_list = plist\n",
    "    return library, pred_list\n",
    "\n",
    "def select_C6(library, pred_list):\n",
    "    \"\"\"Filter library to only include those with one C\"\"\"\n",
    "    seqs = [''.join(l) for l in library]\n",
    "\n",
    "    seq_filter = [(s.count('C') == 1 and s[5] == 'C') for s in seqs]\n",
    "    plist, slist = [], []\n",
    "    for s, f, p in zip(seqs, seq_filter, pred_list):\n",
    "        if f:\n",
    "            slist.append(s)\n",
    "            plist.append(p)\n",
    "    \n",
    "    # filter out any with K or R included\n",
    "    print('%s seqs passed flagged residue filters' % str(len(slist)))\n",
    "    library = slist\n",
    "    pred_list = plist\n",
    "    return library, pred_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fcc419",
   "metadata": {},
   "source": [
    "# Peptide set 1 - 2x Cysteines, predicted highly favorable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917975c7-3e83-48b6-b074-47c7646edc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG - Peptide set 1 (2Cys)\n",
    "config = {\n",
    "    'features': 'onehot', \n",
    "    'model': 'MLP', \n",
    "    'learning_rate': 0.01, \n",
    "    'routine': 'screen', \n",
    "    'ckpt': 'model_weights/LynD_NNK7_rd3.ckpt', \n",
    "    'variable_region': [14, 15, 16, 17, 18, 19, 20, 21], \n",
    "    'variable_length': 8, \n",
    "    'filter_seq': None, \n",
    "    'hamming': 2, \n",
    "    'thresh': 0.97, \n",
    "    'sele': '/proj/kuhl_lab/users/dieckhau/LynD-substrate-modeling/LynD-substrate-modeling/datasets/rd3/sele', \n",
    "    'anti': '/proj/kuhl_lab/users/dieckhau/LynD-substrate-modeling/LynD-substrate-modeling/datasets/rd3/anti', \n",
    "    'seed': 1234, \n",
    "    'include_C': True, \n",
    "    'activity': 'high'\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**config)\n",
    "\n",
    "def nominate_2Cys(args):\n",
    "    \"\"\"Full nomination routine for Peptide Set 1 (2xCys) \"\"\"\n",
    "\n",
    "    # screen 1M random peptides (takes ~10 sec)\n",
    "    library, pred_list = screen_routine(args, N=1000000, thresh=args.thresh, H=args.hamming, save=False, length=args.variable_length, filter=args.filter_seq)\n",
    "    \n",
    "    # SELECTION CRITERIA: select those with 2x Cys in positions 5-8\n",
    "    library, pred_list = select_2Cys(library, pred_list)\n",
    "\n",
    "    # filter by Hamming cutoff (may take a few minutes)\n",
    "    df, ref = hamming(args, library, pred_list)\n",
    "    \n",
    "    # Calculate S scores for rd3 sequences and add them to dataframe\n",
    "    ref_df = ref.df\n",
    "    sel_df = ref_df.loc[ref_df['active'] == 1]\n",
    "    anti_df = ref_df.loc[ref_df['active'] == 0]\n",
    "    \n",
    "    y_star = y_star_score(sel_df, anti_df)\n",
    "    sel_s = s_score(sel_df, y_star)\n",
    "    anti_s = s_score(anti_df, y_star)\n",
    "    \n",
    "    df['var_seq'] = df.SEQ\n",
    "    pep_s = s_score(df, y_star)\n",
    "    \n",
    "    # show sel/anti s scores compared to nominated peptides\n",
    "    \n",
    "    disp = pd.concat([\n",
    "        pd.DataFrame({'Dataset': 'Selection', 'S Score': sel_s}), \n",
    "        pd.DataFrame({'Dataset': 'Antiselection', 'S Score': anti_s}),\n",
    "        pd.DataFrame({'Dataset': 'Nomination', 'S Score': pep_s})\n",
    "                      \n",
    "    ])\n",
    "    disp = disp.reset_index(drop=True)\n",
    "    \n",
    "    ax = sns.kdeplot(data=disp, x='S Score', hue='Dataset', common_norm=False, fill=True)\n",
    "    sns.move_legend(ax, 'upper left')\n",
    "    \n",
    "    # convert S Scores to percentiles for better interpretability (0 to 1)\n",
    "    s_total = np.concatenate([sel_s, anti_s])\n",
    "    pep_s_perc = [(p > s_total).sum() / s_total.size for p in pep_s]\n",
    "    \n",
    "    # save df to output\n",
    "    df['S_SCORE'] = pep_s\n",
    "    df['S_SCORE_PERCENTILE'] = pep_s_perc\n",
    "    df.to_csv('NNK7_NOMINATION_2Cys.csv')\n",
    "\n",
    "nominate_2Cys(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c4106f",
   "metadata": {},
   "source": [
    "# Peptide Set 2: Distance Dependence validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c753bf-71a4-420f-bae2-1225626360a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG - CUSTOM SCREENING TEST\n",
    "\n",
    "config = {\n",
    "    'features': 'onehot', \n",
    "    'model': 'MLP', \n",
    "    'learning_rate': 0.01, \n",
    "    'routine': 'screen', \n",
    "    'ckpt': 'model_weights/LynD_NNK7_rd3.ckpt', \n",
    "    'variable_region': [14, 15, 16, 17, 18, 19, 20, 21], \n",
    "    'variable_length': 8, \n",
    "    'filter_seq': None, \n",
    "    'hamming': -1, \n",
    "    'thresh': 0.97, \n",
    "    'sele': '/proj/kuhl_lab/users/dieckhau/LynD-substrate-modeling/LynD-substrate-modeling/datasets/rd3/sele', \n",
    "    'anti': '/proj/kuhl_lab/users/dieckhau/LynD-substrate-modeling/LynD-substrate-modeling/datasets/rd3/anti', \n",
    "    'seed': 1234, \n",
    "    'include_C': True, \n",
    "    'activity': 'high'\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**config)\n",
    "\n",
    "from trainer import SequenceModelPL\n",
    "from dataset import LibraryDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def screen_custom(args):\n",
    "\n",
    "    # make custom peptide set\n",
    "\n",
    "    # retrieve model and run predictions\n",
    "    model = SequenceModelPL.load_from_checkpoint(args.ckpt, args=args).model\n",
    "    model = model.to('cuda')\n",
    "    model.eval()\n",
    "\n",
    "    # define custom peptides to test\n",
    "    library = [\n",
    "        'ACMSGSGS',\n",
    "        'SACMSGSG',\n",
    "        'GSACMSGS',\n",
    "        'SGSACMSG', \n",
    "        'GSGSACMS', \n",
    "        'SGSGSACM'\n",
    "    ]\n",
    "\n",
    "    tags = [\n",
    "        'pep3', \n",
    "        'pep4', \n",
    "        'pep5', \n",
    "        'pep6', \n",
    "        'pep7', \n",
    "        'pep8'\n",
    "    ]\n",
    "        \n",
    "    library = [list(word) for word in library]\n",
    "    library = np.array(library)\n",
    "\n",
    "    pred_list = []\n",
    "    # screen custom dataset    \n",
    "    ds = LibraryDataset(library, features=args.features)\n",
    "    loader = DataLoader(ds, num_workers=8, shuffle=False, batch_size=1024)\n",
    "    for batch in tqdm(loader):\n",
    "        batch = batch.to('cuda') \n",
    "        preds = model(batch)\n",
    "        preds = torch.squeeze(preds, -1)\n",
    "        pred_list.append(preds) \n",
    "    pred_list = torch.cat(pred_list, dim=0).to('cpu').detach().numpy()\n",
    "    print(pred_list)\n",
    "\n",
    "    library = [''.join(li) for li in library]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'SEQ': library, \n",
    "        'MODEL_SCORE': pred_list\n",
    "    })\n",
    "    \n",
    "    # S-score calculation\n",
    "    ref = hamming(args, library, pred_list)\n",
    "    ref_df = ref.df\n",
    "    sel_df = ref_df.loc[ref_df['active'] == 1]\n",
    "    anti_df = ref_df.loc[ref_df['active'] == 0]\n",
    "    \n",
    "    y_star = y_star_score(sel_df, anti_df)\n",
    "    sel_s = s_score(sel_df, y_star)\n",
    "    anti_s = s_score(anti_df, y_star)\n",
    "    \n",
    "    df['var_seq'] = df.SEQ\n",
    "    pep_s = s_score(df, y_star)\n",
    "\n",
    "    # visualize S scores\n",
    "    disp = pd.concat([\n",
    "        pd.DataFrame({'Dataset': 'Selection', 'S Score': sel_s}), \n",
    "        pd.DataFrame({'Dataset': 'Antiselection', 'S Score': anti_s}),\n",
    "        pd.DataFrame({'Dataset': 'Nomination', 'S Score': pep_s})\n",
    "                      \n",
    "    ])\n",
    "    disp = disp.reset_index(drop=True)\n",
    "    \n",
    "    ax = sns.kdeplot(data=disp, x='S Score', hue='Dataset', common_norm=False, fill=True)\n",
    "    sns.move_legend(ax, 'upper left')\n",
    "    \n",
    "    # convert S Scores to percentiles for better interpretability (0 to 1)\n",
    "    s_total = np.concatenate([sel_s, anti_s])\n",
    "    pep_s_perc = [(p > s_total).sum() / s_total.size for p in pep_s]\n",
    "    \n",
    "    # save df to output\n",
    "    df['S_SCORE'] = pep_s\n",
    "    df['S_SCORE_PERCENTILE'] = pep_s_perc\n",
    "    df['TAG'] = tags\n",
    "    df.to_csv('NNK7_DistanceDependence.csv')\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = screen_custom(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b49000",
   "metadata": {},
   "source": [
    "# Peptide Set 3 - C6 Peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3bccef-0037-484b-ab60-c3968e325d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG - Peptide set 3 (C6Range)\n",
    "\n",
    "config = {\n",
    "    'features': 'onehot', \n",
    "    'model': 'MLP', \n",
    "    'learning_rate': 0.01, \n",
    "    'routine': 'screen', \n",
    "    'ckpt': 'model_weights/LynD_NNK7_rd3.ckpt', \n",
    "    'variable_region': [14, 15, 16, 17, 18, 19, 20, 21], \n",
    "    'variable_length': 8, \n",
    "    'filter_seq': 'C6', \n",
    "    'hamming': 2, \n",
    "    'thresh': 0.03, \n",
    "    'sele': '/proj/kuhl_lab/users/dieckhau/LynD-substrate-modeling/LynD-substrate-modeling/datasets/rd3/sele', \n",
    "    'anti': '/proj/kuhl_lab/users/dieckhau/LynD-substrate-modeling/LynD-substrate-modeling/datasets/rd3/anti', \n",
    "    'seed': 1234, \n",
    "    'include_C': True, \n",
    "    'activity': 'low'\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**config)\n",
    "\n",
    "from trainer import SequenceModelPL\n",
    "from dataset import LibraryDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def nominate_C6Range(args):\n",
    "\n",
    "    # generate 1M peptides with high predicted activity\n",
    "    library, pred_list = screen_routine(args, N=1000000, thresh=args.thresh, H=args.hamming, save=False, length=args.variable_length, filter=args.filter_seq)    \n",
    "\n",
    "    library, pred_list = select_C6_only(library, pred_list)\n",
    "\n",
    "    # keep only mid-range values (0.1 to 0.9)\n",
    "    # library_keep, pred_keep = [], []\n",
    "    # for li, pr, in zip(library, pred_list):\n",
    "    #     if (pr > 0.1) and (pr < 0.9):\n",
    "    #         library_keep.append(li)\n",
    "    #         pred_keep.append(pr)\n",
    "    # library = library_keep\n",
    "    # pred_list = pred_keep\n",
    "    \n",
    "    # S-score calculation\n",
    "    df, ref = hamming(args, library, pred_list)\n",
    "    ref_df = ref.df\n",
    "    sel_df = ref_df.loc[ref_df['active'] == 1]\n",
    "    anti_df = ref_df.loc[ref_df['active'] == 0]\n",
    "    \n",
    "    y_star = y_star_score(sel_df, anti_df)\n",
    "    sel_s = s_score(sel_df, y_star)\n",
    "    anti_s = s_score(anti_df, y_star)\n",
    "    \n",
    "    df['var_seq'] = df.SEQ\n",
    "    pep_s = s_score(df, y_star)\n",
    "\n",
    "    # visualize S scores\n",
    "    disp = pd.concat([\n",
    "        pd.DataFrame({'Dataset': 'Selection', 'S Score': sel_s}), \n",
    "        pd.DataFrame({'Dataset': 'Antiselection', 'S Score': anti_s}),\n",
    "        pd.DataFrame({'Dataset': 'Nomination', 'S Score': pep_s})\n",
    "                      \n",
    "    ])\n",
    "    disp = disp.reset_index(drop=True)\n",
    "    \n",
    "    ax = sns.kdeplot(data=disp, x='S Score', hue='Dataset', common_norm=False, fill=True)\n",
    "    sns.move_legend(ax, 'upper left')\n",
    "    \n",
    "    # convert S Scores to percentiles for better interpretability (0 to 1)\n",
    "    s_total = np.concatenate([sel_s, anti_s])\n",
    "    pep_s_perc = [(p > s_total).sum() / s_total.size for p in pep_s]\n",
    "    \n",
    "    # save df to output\n",
    "    df['S_SCORE'] = pep_s\n",
    "    df['S_SCORE_PERCENTILE'] = pep_s_perc\n",
    "    df.to_csv('NNK7_C6Range.csv')\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = nominate_C6Range(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proteinMPNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
